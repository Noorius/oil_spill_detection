{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os \n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.functional import img_to_tensor\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.config import load_config\n",
    "from helpers.utils import create_optimizer, AverageMeter\n",
    "from helpers.losses import miou_round, val_miou_round\n",
    "from helpers import losses\n",
    "conf = load_config('helpers/eff_conf.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTH = '../../oil_spill_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = os.path.join(PTH, 'train/images')\n",
    "train_mask = os.path.join(PTH, 'train/labels')\n",
    "\n",
    "valid = os.path.join(PTH, 'test/images')\n",
    "valid_mask = os.path.join(PTH, 'test/labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_transforms():\n",
    "    #(650,1250) > 512x512 > 512x512 > (650x1250) > \n",
    "    \n",
    "    return A.Compose([\n",
    "        A.RandomCrop(640, 640, p=1.0),\n",
    "        #A.ShiftScaleRotate(),\n",
    "        A.HorizontalFlip()\n",
    "    ])\n",
    "\n",
    "def create_val_transforms():\n",
    "    return A.Compose([\n",
    "#        A.Crop(0,0,640,640, p=1.0)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert RGB mask to 1D mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_label(rgb_mask):\n",
    "    color_to_label = {\n",
    "        (0,0,0): 0,\n",
    "        (0,255,255): 1,\n",
    "        (255,0,0): 2,\n",
    "        (153, 76,0): 3,\n",
    "        (0,153,0): 4\n",
    "    }\n",
    "    height, width, _ = rgb_mask.shape\n",
    "    label_mask = np.zeros((height, width), dtype = np.uint8)\n",
    "    \n",
    "    for color, label in color_to_label.items():\n",
    "        indices = np.where(np.all(rgb_mask==color, axis=-1))\n",
    "        #print(indices)\n",
    "        label_mask[indices] = label\n",
    "    return label_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mask = cv2.imread('../../oil_spill_dataset/train/labels/34_904,35_492_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mask = rgb_to_label(rgb_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV with common file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_ims(img_pth, mask_pth, mode):\n",
    "    img_ids = os.listdir(img_pth)\n",
    "    mask_ids = os.listdir(mask_pth)\n",
    "    mask_ids = [s.replace('png', 'jpg') for s in mask_ids]\n",
    "    \n",
    "    img_df = pd.DataFrame({'filename': img_ids})\n",
    "    mask_df = pd.DataFrame({'filename': mask_ids})\n",
    "\n",
    "    merged_df = pd.merge(img_df, mask_df, on='filename')\n",
    "    merged_df.to_csv(f'{mode}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ims(train, train_mask, 'train')\n",
    "common_ims(valid, valid_mask, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')[:50]\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1014, 1022)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(train)), len(os.listdir(train_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OilDataset(Dataset):\n",
    "    def __init__(self, df, mode='train', classes=None, augmentations = None, normalize = None):\n",
    "        self.df = df['filename']\n",
    "        self.mode = mode\n",
    "        self.classes = classes\n",
    "        self.augmentations = augmentations\n",
    "        self.normalize = normalize\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(os.path.join(f'../../oil_spill_dataset/{self.mode}/images/', self.df[idx]))\n",
    "        mask = cv2.imread(os.path.join(f'../../oil_spill_dataset/{self.mode}/labels/', self.df[idx].replace('jpg', 'png')))\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = rgb_to_label(mask)\n",
    "        mask[mask != 1] = 0\n",
    "        \n",
    "        if self.augmentations:\n",
    "            sample = self.augmentations(image=image, mask=mask)\n",
    "            #image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        mask = np.zeros((self.classes, *sample[\"mask\"].shape[:2]))\n",
    "        for i in range(self.classes):\n",
    "            mask[i, sample[\"mask\"] == i] = 1\n",
    "        \n",
    "        sample['img_name'] = os.path.join(f'../../oil_spill_dataset/{self.mode}/images/', self.df[idx])\n",
    "        sample['mask_orig'] = sample['mask']\n",
    "        sample['mask'] = torch.from_numpy(np.ascontiguousarray(mask)).float()\n",
    "        sample['image'] = img_to_tensor(np.ascontiguousarray(sample['image']), self.normalize)\n",
    "        return sample\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OilDataset(train_df, mode='train', classes = 2, augmentations=create_train_transforms(), normalize = conf['input']['normalize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 640, 640]),\n",
       " '../../oil_spill_dataset/train/images/43_538,29_653_2.jpg',\n",
       " torch.Size([2, 640, 640]),\n",
       " (640, 640))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['image'].shape, dataset[0]['img_name'], dataset[0]['mask'].shape, dataset[0]['mask_orig'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dataset[0]['mask_orig'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(current_epoch, loss_functions, model, optimizer, scheduler, train_data_loader, \n",
    "                summary_writer, conf):\n",
    "    losses = AverageMeter()\n",
    "    mious = AverageMeter()\n",
    "    iterator = tqdm(train_data_loader)\n",
    "    model.train()\n",
    "    if conf[\"optimizer\"][\"schedule\"][\"mode\"] == \"epoch\":\n",
    "        scheduler.step(current_epoch)\n",
    "    for i, sample in enumerate(iterator):\n",
    "        imgs = sample[\"image\"].cuda()\n",
    "        masks = sample[\"mask\"].cuda().float()\n",
    "        masks_orig = sample[\"mask_orig\"].cuda().float()\n",
    "        out_mask = model(imgs)\n",
    "        with torch.no_grad():\n",
    "            pred = torch.softmax(out_mask, dim=1)\n",
    "            argmax = torch.argmax(pred, dim=1)\n",
    "            ious = miou_round(argmax, masks_orig).item()\n",
    "\n",
    "        mious.update(ious, imgs.size(0))\n",
    "\n",
    "        mask_loss = loss_functions[\"mask_loss\"](out_mask, masks.contiguous())\n",
    "        loss = mask_loss\n",
    "        losses.update(loss.item(), imgs.size(0))\n",
    "        iterator.set_description(\n",
    "            \"epoch: {}; lr {:.7f}; Loss ({loss.avg:.4f}); miou ({miou.avg:.4f}); \".format(\n",
    "                current_epoch, scheduler.get_lr()[-1], loss=losses, miou=mious))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        #torch.cuda.synchronize()\n",
    "\n",
    "        if conf[\"optimizer\"][\"schedule\"][\"mode\"] in (\"step\", \"poly\"):\n",
    "            scheduler.step(i + current_epoch * len(train_data_loader))\n",
    "   \n",
    "    for idx, param_group in enumerate(optimizer.param_groups):\n",
    "        lr = param_group['lr']\n",
    "        summary_writer.add_scalar('group{}/lr'.format(idx), float(lr), global_step=current_epoch)\n",
    "    summary_writer.add_scalar('train/loss', float(losses.avg), global_step=current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_single_image(image, model, conf):\n",
    "           \n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        image = img_to_tensor(np.ascontiguousarray(image), conf['input']['normalize'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            imgs = torch.unsqueeze(image, 0).cuda().float()\n",
    "            output = model(imgs)\n",
    "            pred = torch.softmax(output, dim=1)\n",
    "            mask = pred.squeeze(0).permute(1,2,0).cpu().numpy().astype(np.float)\n",
    "        return mask\n",
    "\n",
    "def cropper_validator(img, model, conf):\n",
    "    \n",
    "    # Define the patch size and overlap\n",
    "    patch_size = (640, 640)\n",
    "    overlap = 0.7\n",
    "    #print(img.shape)\n",
    "    # Load the semantic segmentation model\n",
    "    segmentation_map = np.zeros((img.shape[0], img.shape[1], 2), dtype=np.float32)\n",
    "    #print(segmentation_map.shape)\n",
    "    # Loop over the image patches\n",
    "    for y in range(0, img.shape[0], int(patch_size[0] * overlap)):\n",
    "        for x in range(0, img.shape[1], int(patch_size[1] * overlap)):\n",
    "\n",
    "            patch = img[y:y+patch_size[0], x:x+patch_size[1]]\n",
    "            \n",
    "            if patch.shape[1] < patch_size[1] and patch.shape[0] == patch_size[0]:\n",
    "                patch = img[y:y+patch_size[0], img.shape[1]-patch_size[1]:img.shape[1]]\n",
    "                patch_seg_map = segment_single_image(patch, model, conf)\n",
    "                segmentation_map[y:y+patch_size[0], img.shape[1]-patch_size[1]:img.shape[1]] += patch_seg_map\n",
    "                #print(segmentation_map.shape)\n",
    "            elif patch.shape[0] < patch_size[0] and patch.shape[1] == patch_size[1]:\n",
    "                patch = img[img.shape[0]-patch_size[0]:img.shape[0], x:x+patch_size[1]]\n",
    "                patch_seg_map = segment_single_image(patch, model, conf)\n",
    "                #patch_seg_map = segmenter.segment_single_image(patch, 'img')\n",
    "                segmentation_map[img.shape[0]-patch_size[0]:img.shape[0], x:x+patch_size[1]] += patch_seg_map\n",
    "            elif patch.shape[0] < patch_size[0] and patch.shape[1] < patch_size[1]:\n",
    "                patch = img[img.shape[0]-patch_size[0]:img.shape[0], img.shape[1]-patch_size[1]:img.shape[1]]\n",
    "                #patch_seg_map = segmenter.segment_single_image(patch, 'img')\n",
    "                patch_seg_map = segment_single_image(patch, model, conf)\n",
    "                segmentation_map[img.shape[0]-patch_size[0]:img.shape[0], img.shape[1]-patch_size[1]:img.shape[1]] += patch_seg_map\n",
    "            else:\n",
    "                patch_seg_map = segment_single_image(patch, model, conf)\n",
    "                segmentation_map[y:y+patch_size[0], x:x+patch_size[1]] += patch_seg_map\n",
    "    \n",
    "    return segmentation_map #probability map 2x650x1250 (0,...)\n",
    "\n",
    "def validate(net, predictions_dir, conf):\n",
    "    os.makedirs(predictions_dir, exist_ok=True)\n",
    "    preds_dir = predictions_dir + \"/predictions\"\n",
    "    os.makedirs(preds_dir, exist_ok=True)\n",
    "    mious = []\n",
    "    oil_class_iou = []\n",
    "    with torch.no_grad():\n",
    "        for filename in test_df.filename:\n",
    "            \n",
    "            image = cv2.imread(os.path.join(f'../../oil_spill_dataset/test/images/', filename))\n",
    "            mask = cv2.imread(os.path.join(f'../../oil_spill_dataset/test/labels/', filename.replace('jpg', 'png')))\n",
    "            mask = rgb_to_label(mask)\n",
    "            mask[mask != 1] = 0\n",
    "            \n",
    "            mask = torch.tensor(mask) \n",
    "            \n",
    "            map_preds = cropper_validator(image, net, conf)\n",
    "            output = torch.tensor(map_preds).permute(2,0,1)\n",
    "            #print(map_preds.shape)\n",
    "\n",
    "            argmax = torch.argmax(output, dim=0) # 650x1250 #0,1,1,,0,1,1, 0\n",
    "            \n",
    "            #print(argmax.shape, mask.shape)\n",
    "            for i in range(output.shape[0]):\n",
    "                d, ious = val_miou_round(argmax, mask)\n",
    "                mious.append(d.item())\n",
    "                oil_class_iou.append(ious[1].item())\n",
    "\n",
    "    print(np.mean(oil_class_iou))\n",
    "    return np.mean(mious)\n",
    "def evaluate_val(output_dir, data_val, miou_best, model, snapshot_name, current_epoch, optimizer, summary_writer,\n",
    "                 predictions_dir, conf):\n",
    "    print(\"Test phase\")\n",
    "    model = model.eval()\n",
    "        \n",
    "    miou = validate(model, predictions_dir, conf)\n",
    "    summary_writer.add_scalar('val/miou', float(miou), global_step=current_epoch)\n",
    "    if miou > miou_best:\n",
    "        if output_dir is not None:\n",
    "            torch.save({\n",
    "                'epoch': current_epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'miou_best': miou,\n",
    "\n",
    "            }, output_dir + '/'+snapshot_name + \"_best_miou.pt\")\n",
    "        miou_best = miou\n",
    "    torch.save({\n",
    "        'epoch': current_epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'miou_best': miou_best,\n",
    "    }, output_dir +'/' + snapshot_name + \"_last.pt\")\n",
    "    print(\"miou: {}, miou_best: {}\".format(miou, miou_best))\n",
    "    return miou_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "model = smp.Unet(encoder_name='efficientnet-b4', classes=2) #, encoder_weights=None)\n",
    "# model = smp.Unet(encoder_name='efficientnet-b4', classes=5, encoder_weights=None)\n",
    "# checkpoint = torch.load('best_oil.pt', map_location='cpu')\n",
    "# state_dict = checkpoint['state_dict']\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "# model.segmentation_head[0] = torch.nn.Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.segmentation_head[0] = torch.nn.Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_loss_function = losses.__dict__[conf[\"mask_loss\"][\"type\"]](**conf[\"mask_loss\"][\"params\"]).cuda()\n",
    "loss_functions = {\"mask_loss\": mask_loss_function}\n",
    "optimizer, scheduler = create_optimizer(conf['optimizer'], model)\n",
    "\n",
    "miou_best = 0\n",
    "start_epoch = 0\n",
    "batch_size = conf['optimizer']['batch_size']\n",
    "#print(train_lst)\n",
    "data_train = OilDataset(train_df,\n",
    "                        mode='train', \n",
    "                        classes = 2, \n",
    "                        augmentations=create_train_transforms(), \n",
    "                        normalize = conf['input']['normalize'])\n",
    "data_val = OilDataset(test_df,\n",
    "                        mode='test', \n",
    "                        classes = 2, \n",
    "                        augmentations=create_val_transforms(), \n",
    "                        normalize = conf['input']['normalize'])\n",
    "train_sampler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(data_train, batch_size=batch_size, num_workers=8,\n",
    "                                   shuffle=train_sampler is None, sampler=train_sampler, pin_memory=False,\n",
    "                                   drop_last=True)\n",
    "#print(data_train.__getitem__(1)['image'].shape)\n",
    "#print(data_train.__getitem__(1)['mask'].shape)\n",
    "val_batch_size = 1\n",
    "val_data_loader = DataLoader(data_val, batch_size=val_batch_size, num_workers=8, shuffle=False,\n",
    "                             pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'outputs'\n",
    "predictions_path = 'predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'logs'\n",
    "summary_writer = SummaryWriter(logdir + '/' + 'segment_' + conf['encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "current_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_name = \"{}{}_{}\".format('segment_', conf['network'], conf['encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, conf['optimizer']['schedule']['epochs']):\n",
    "        if train_sampler:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "\n",
    "        model_encoder_stages = model.encoder\n",
    "        model_encoder_stages.train()\n",
    "        for p in model_encoder_stages.parameters():\n",
    "            p.requires_grad = True\n",
    "        train_epoch(current_epoch, loss_functions, model, \n",
    "                    optimizer, scheduler, train_data_loader, summary_writer, conf)\n",
    "\n",
    "        model = model.eval()\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': current_epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'miou_best': miou_best,\n",
    "        }, output_dir + '/' + snapshot_name + \"_last\")\n",
    "        preds_dir = os.path.join(predictions_path, snapshot_name)\n",
    "        miou_best =  evaluate_val(output_dir, val_data_loader, miou_best, model, snapshot_name,\n",
    "                      current_epoch,\n",
    "                      optimizer, summary_writer,\n",
    "                      preds_dir, conf)\n",
    "        current_epoch += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1d to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def convert_mask(mask):\n",
    "   \n",
    "    color_mappings = {\n",
    "        0: (0, 0, 0),\n",
    "        1: (0, 255, 255),\n",
    "        2: (255, 0, 0),\n",
    "        3: (153, 76, 0),\n",
    "        4: (0, 153, 0)\n",
    "    }\n",
    "\n",
    "   \n",
    "    height, width= mask.shape\n",
    "    rgb_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "   \n",
    "    for class_idx, color in color_mappings.items():\n",
    "        class_mask = (mask == class_idx)\n",
    "        rgb_mask[class_mask] = color\n",
    "\n",
    "    \n",
    "    return rgb_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('predictions/segment_resnet34_resnet34/predictions/test_42_308,18_108_1.jpg_mask_orig.png')\n",
    "img2 = cv2.imread('predictions/segment_resnet34_resnet34/predictions/test_42_308,18_108_1.jpg_prediction.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = img1/64\n",
    "img2 = img2/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(convert_mask(img1[:,:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(convert_mask(img2[:,:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[:,:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 650x1250 > "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch11_env)",
   "language": "python",
   "name": "torch11_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
